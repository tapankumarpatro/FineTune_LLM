{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install langchain huggingface_hub transformers sentence_transformers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install accelerate \n",
    "# !pip install --upgrade accelerate\n",
    "!pip install --upgrade transformers\n",
    "#bitsandbytes\n",
    "# !pip install -i https://test.pypi.org/simple/ bitsandbytes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat May  6 16:52:01 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 526.47       Driver Version: 526.47       CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ... WDDM  | 00000000:01:00.0  On |                  N/A |\n",
      "|  0%   47C    P8     9W / 170W |    770MiB / 12288MiB |      8%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      2164    C+G   ...in7x64\\steamwebhelper.exe    N/A      |\n",
      "|    0   N/A  N/A      6624    C+G   C:\\Windows\\explorer.exe         N/A      |\n",
      "|    0   N/A  N/A      6808    C+G   ...\\app-1.0.9013\\Discord.exe    N/A      |\n",
      "|    0   N/A  N/A      8868    C+G   ...artMenuExperienceHost.exe    N/A      |\n",
      "|    0   N/A  N/A      8948    C+G   ...n1h2txyewy\\SearchHost.exe    N/A      |\n",
      "|    0   N/A  N/A      9800    C+G   ...2txyewy\\TextInputHost.exe    N/A      |\n",
      "|    0   N/A  N/A     14444    C+G   ...perience\\NVIDIA Share.exe    N/A      |\n",
      "|    0   N/A  N/A     15192    C+G   ...oft\\OneDrive\\OneDrive.exe    N/A      |\n",
      "|    0   N/A  N/A     16152    C+G   ...8wekyb3d8bbwe\\GameBar.exe    N/A      |\n",
      "|    0   N/A  N/A     17784    C+G   ...722.68\\msedgewebview2.exe    N/A      |\n",
      "|    0   N/A  N/A     18680    C+G   ...icrosoft VS Code\\Code.exe    N/A      |\n",
      "|    0   N/A  N/A     18724    C+G   ...y\\ShellExperienceHost.exe    N/A      |\n",
      "|    0   N/A  N/A     18936    C+G   ...n64\\EpicGamesLauncher.exe    N/A      |\n",
      "|    0   N/A  N/A     19304    C+G   ...s\\Win64\\EpicWebHelper.exe    N/A      |\n",
      "|    0   N/A  N/A     20304    C+G   ...8wekyb3d8bbwe\\msteams.exe    N/A      |\n",
      "|    0   N/A  N/A     21312    C+G   ...722.68\\msedgewebview2.exe    N/A      |\n",
      "|    0   N/A  N/A     22364    C+G   ...me\\Application\\chrome.exe    N/A      |\n",
      "|    0   N/A  N/A     24012    C+G   ...bbwe\\Microsoft.Photos.exe    N/A      |\n",
      "|    0   N/A  N/A     24084    C+G   ...v1g1gvanyjgm\\WhatsApp.exe    N/A      |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)okenizer_config.json: 100%|██████████| 1.05k/1.05k [00:00<?, ?B/s]\n",
      "Downloading (…)lve/main/config.json: 100%|██████████| 1.38k/1.38k [00:00<?, ?B/s]\n",
      "Downloading (…)olve/main/vocab.json: 100%|██████████| 127k/127k [00:00<00:00, 569kB/s]\n",
      "Downloading (…)olve/main/merges.txt: 100%|██████████| 62.9k/62.9k [00:00<00:00, 577kB/s]\n",
      "Downloading (…)in/added_tokens.json: 100%|██████████| 16.0/16.0 [00:00<?, ?B/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|██████████| 772/772 [00:00<?, ?B/s] \n",
      "Downloading pytorch_model.bin:  13%|█▎        | 388M/2.87G [02:23<16:25, 2.52MB/s] "
     ]
    }
   ],
   "source": [
    "## Local model\n",
    "\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, AutoModelForSeq2SeqLM\n",
    "from accelerate import init_empty_weights\n",
    "\n",
    "\n",
    "# with init_empty_weights():\n",
    "#     model = AutoModelForCausalLM.from_config(config)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_id = 'facebook/blenderbot-1B-distill' #'google/flan-t5-large' \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_id) #, torch_dtype=torch.float16, load_in_8bit=False, device_map='auto')\n",
    "model.to(device)\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text2text-generation\",\n",
    "    model=model, \n",
    "    tokenizer=tokenizer, \n",
    "    max_length=100\n",
    ")\n",
    "\n",
    "local_llm = HuggingFacePipeline(pipeline=pipe)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "paris\n"
     ]
    }
   ],
   "source": [
    "print(local_llm('What is the capital of France? '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chmod is a command line utility that allows you to change the permissions of a file. The answer: chmod.\n"
     ]
    }
   ],
   "source": [
    "from langchain import PromptTemplate, HuggingFaceHub, LLMChain\n",
    "\n",
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's think step by step.\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "\n",
    "llm_chain = LLMChain(prompt=prompt, \n",
    "                     llm=local_llm\n",
    "                     )\n",
    "\n",
    "question = \"Code for giving full permission with chmod\" #\"What is the capital of England?\"\n",
    "\n",
    "print(llm_chain.run(question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
